from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms.mlx_pipeline import MLXPipeline
from langchain_community.chat_models.mlx import ChatMLX
from image_agent.models.config import llama_path
from typing import Any


class LlamaCaller:
    """
    A class to interact with the Llama model for generating responses based on a system prompt.

    Attributes:
        MODEL_PATH (str): The path to the Llama model.
        system_prompt (Any): The system prompt used for the model.
        loaded_model (MLXPipeline): The loaded model pipeline.
        llm (ChatMLX): The chat model for generating responses.
        temperature (float): The sampling temperature for response generation.
        max_tokens (int): The maximum number of tokens to generate in a response.
        chain (Any): The processing chain for handling queries.
    """

    MODEL_PATH = llama_path

    def __init__(
        self, system_prompt: Any, temperature: float = 0, max_tokens: int = 1000
    ) -> None:
        """
        Initializes the LlamaCaller with the specified system prompt, temperature, and max tokens.

        Args:
            system_prompt (Any): The system prompt for the model.
            temperature (float): The sampling temperature for response generation.
            max_tokens (int): The maximum number of tokens to generate in a response.
        """
        self.system_prompt: Any = system_prompt
        self.loaded_model: MLXPipeline = MLXPipeline.from_model_id(
            self.MODEL_PATH,
            pipeline_kwargs={
                "max_tokens": max_tokens,
                "temp": temperature,
                "do_sample": False,
            },
        )
        self.llm: ChatMLX = ChatMLX(llm=self.loaded_model)
        self.temperature: float = temperature
        self.max_tokens: int = max_tokens
        self.chain: Any = self._set_up_chain()

    def _set_up_chain(self) -> Any:
        """
        Sets up the processing chain for handling queries.

        Returns:
            Any: The configured processing chain.
        """
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt.system_template),
                ("human", "{query}"),
            ]
        )
        chain = prompt | self.llm | StrOutputParser()
        return chain

    def call(self, query: str) -> Any:
        """
        Invokes the model with the specified query.

        Args:
            query (str): The query to process.

        Returns:
            Any: The response generated by the model.
        """
        return self.chain.invoke({"query": query})


class StructuredLlamaCaller(LlamaCaller):
    """
    A class to interact with the Llama model for generating structured responses based on a system prompt.

    Inherits from LlamaCaller and adds structured output capabilities.

    Attributes:
        output_model (Any): The output model for structured responses.
    """

    def __init__(
        self,
        system_prompt: Any,
        output_model: Any,
        temperature: float = 0,
        max_tokens: int = 1000,
    ) -> None:
        """
        Initializes the StructuredLlamaCaller with the specified system prompt, output model, temperature, and max tokens.

        Args:
            system_prompt (Any): The system prompt for the model.
            output_model (Any): The output model for structured responses.
            temperature (float): The sampling temperature for response generation.
            max_tokens (int): The maximum number of tokens to generate in a response.
        """
        self.output_model: Any = output_model
        self.chain: Any = self._set_up_chain()

    def _set_up_chain(self) -> Any:
        """
        Sets up the processing chain for handling structured queries.

        Returns:
            Any: The configured processing chain with structured output.
        """
        # Set up a parser
        parser = PydanticOutputParser(pydantic_object=self.output_model)

        # Prompt
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    self.system_prompt.system_template,
                ),
                ("human", "{query}"),
            ]
        ).partial(format_instructions=parser.get_format_instructions())

        chain = prompt | self.llm | parser
        return chain
